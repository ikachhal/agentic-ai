{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XB8V_Ez1HQvZ",
        "outputId": "51b51316-a39d-4a23-edbe-8d710f1cab22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing .env\n"
          ]
        }
      ],
      "source": [
        "%%writefile .env\n",
        "#OPENAI_API_KEY=\"sk-proj-\"\n",
        "#TAVILY_API_KEY=tvly-dev-"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtnH0ZMSYZH9",
        "outputId": "6180f2c1-5d23-4ab1-9603-b6c42e4edc8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Skipping openai-agents as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "pip uninstall -y openai-agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "vgAVj-RjYzBo",
        "outputId": "73b92422-98fe-4592-d9fc-d5fb24ba92f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/161.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m161.1/161.1 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/144.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m143.4/144.4 kB\u001b[0m \u001b[31m137.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m144.4/144.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "pip install -q openai-agents==0.2.2 python-dotenv requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "id": "Cj4g7yHTaJKI"
      },
      "outputs": [],
      "source": [
        "pip install python-dotenv langchain-openai==0.2.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "zrT7Syw1aOrR"
      },
      "outputs": [],
      "source": [
        "pip install --upgrade pydantic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOhF2DvZaWLY",
        "outputId": "93c74473-e098-4030-b03b-a6315643352a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI client successfully configured.\n",
            "sk-pr\n",
            "tvly-\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# This will be used to load the API key from the .env file\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "# Get the OpenAI API keys from environment variables\n",
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "tavily_api_key = os.getenv(\"TAVILY_API_KEY\")\n",
        "\n",
        "# Let's configure the OpenAI Client using our key\n",
        "openai_client = OpenAI(api_key = openai_api_key)\n",
        "print(\"OpenAI client successfully configured.\")\n",
        "\n",
        "# Let's view the first few characters in the key\n",
        "print(openai_api_key[:5])\n",
        "print(tavily_api_key[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "B1RpGhrBnWwf"
      },
      "outputs": [],
      "source": [
        "def print_markdown(text):\n",
        "    \"\"\"Displays text as Markdown in Jupyter.\"\"\"\n",
        "    display(Markdown(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "yTa2aDjWsM67"
      },
      "outputs": [],
      "source": [
        "# Import the OpenAI API client\n",
        "from openai import OpenAI\n",
        "\n",
        "# Import the Agent class to create and manage AI agents\n",
        "from agents import Agent\n",
        "\n",
        "# Define the instructions for the fact-checker AI Agent\n",
        "fact_checker_instructions = \"\"\"\n",
        "Context:\n",
        "You are a fact-checker who verifies the accuracy of statements.\n",
        "\n",
        "Instructions:\n",
        "When given a statement, carefully analyze its factual accuracy using your knowledge.\n",
        "\n",
        "Input:\n",
        "You will receive a statement that requires fact-checking.\n",
        "\n",
        "Output:\n",
        "Respond with:\n",
        "1. A verdict prefix: either \"âœ… TRUE:\" or \"âŒ FALSE:\"\n",
        "2. A brief, one-sentence explanation justifying your conclusion\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Y5puYbHsS92",
        "outputId": "10de415b-0488-40f3-9b5a-57c910567210"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent 'FactÂ Checker' created successfully!\n"
          ]
        }
      ],
      "source": [
        "# Create a new agent called \"Fact Checker\"\n",
        "fact_checker_agent = Agent(name = \"FactÂ Checker\",   # Name of the agent\n",
        "                           instructions = fact_checker_instructions, # The rules and behavior for the agent\n",
        "                           model = \"gpt-4o-mini\") # The AI model (LLM) to use\n",
        "\n",
        "# Print a confirmation message that the agent was created\n",
        "print(f\"Agent '{fact_checker_agent.name}' created successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "44DIvsklBLzh",
        "outputId": "20b11163-70d7-4e8b-b5f3-994c550a006f"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Asking the Fact Checker to verify: 'The Great Wall of China is not visible from space with the naked eye.'"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "\n",
              "ğŸ¤– Agent's Response:\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "âœ… TRUE: The Great Wall of China is generally not visible from space without aid because it is narrow and blends in with the natural terrain, contrary to popular belief."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Import the Runner class, which is used to run an agent and get its output\n",
        "from agents import Runner\n",
        "\n",
        "# A statement we want the Fact Checker agent to verify\n",
        "statement = \"The Great Wall of China is not visible from space with the naked eye.\"\n",
        "\n",
        "# Display the statement we're going to check (in markdown format for nicer formatting)\n",
        "print_markdown(f\"Asking the Fact Checker to verify: '{statement}'\")\n",
        "\n",
        "# Run the Fact Checker agent on the input statement\n",
        "# 'await' is used because running the agent is an asynchronous operation (it might take time)\n",
        "response = await Runner.run(\n",
        "    starting_agent = fact_checker_agent,  # The agent we created earlier\n",
        "    input = statement                 # The statement we want it to fact-check\n",
        ")\n",
        "\n",
        "# Display the agent's response\n",
        "print_markdown(\"\\nğŸ¤– Agent's Response:\\n\")\n",
        "print_markdown(response.final_output)    # Shows the final verdict and explanation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "F1JHZ9u7hju_"
      },
      "outputs": [],
      "source": [
        "from agents import Agent, Runner, SQLiteSession\n",
        "\n",
        "# Define the role and instructions for the AI agent\n",
        "market_researcher_instructions = \"\"\"\n",
        "Context:\n",
        "You are a market research assistant helping analyze companies, industries, and competitors.\n",
        "\n",
        "Instructions:\n",
        "When given a question, provide a short factual answer based on your knowledge.\n",
        "\n",
        "Output:\n",
        "Start with a verdict prefix: either \"âœ… FACT:\" or \"âŒ UNKNOWN:\"\n",
        "Follow with a concise one-sentence explanation.\n",
        "\"\"\"\n",
        "\n",
        "# Create an instance of the Agent\n",
        "market_researcher_agent = Agent(name = \"Market Researcher\",\n",
        "                                instructions = market_researcher_instructions,\n",
        "                                model = \"gpt-4.1-mini\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "7pycLqcDhtNg",
        "outputId": "57e36f29-99c3-4845-b186-172679f2765b"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "You: 'What is the market share of Tesla in the US EV market?'"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "ğŸ¤– Agent:\n",
              "âœ… FACT: Tesla holds approximately 60-70% of the US electric vehicle (EV) market share as of early 2024."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Let's give our first question to the AI agent\n",
        "q1 = \"What is the market share of Tesla in the US EV market?\"\n",
        "\n",
        "# Display the userâ€™s question in Markdown format\n",
        "print_markdown(f\"You: '{q1}'\")\n",
        "\n",
        "# Run the agent with the first question (no memory means each query is independent)\n",
        "resp1 = await Runner.run(starting_agent = market_researcher_agent, input = q1)\n",
        "\n",
        "# Display the agentâ€™s response\n",
        "print_markdown(f\"ğŸ¤– Agent:\\n{resp1.final_output}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "tmE9a5RKh1A_",
        "outputId": "49dd00b3-a344-4d01-aae0-3bfb4aa89130"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "\n",
              "You: 'How does that compare to last year?'"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "ğŸ¤– Agent:\n",
              "âŒ UNKNOWN: You have not provided the specific data or context needed to compare this year to last year."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Second question â€” depends on previous context\n",
        "# Follow-up question that refers to the previous answer\n",
        "q2 = \"How does that compare to last year?\"\n",
        "\n",
        "# Display the follow-up question\n",
        "print_markdown(f\"\\nYou: '{q2}'\")\n",
        "\n",
        "# Run the agent again â€” since thereâ€™s no memory, it does not recall the first question/answer\n",
        "resp2 = await Runner.run(starting_agent = market_researcher_agent, input = q2)\n",
        "\n",
        "# Display the agentâ€™s response (will fail to connect it to the first question)\n",
        "print_markdown(f\"ğŸ¤– Agent:\\n{resp2.final_output}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "lH7zakeOl3SX",
        "outputId": "1244df1b-808a-4126-efbf-bf4cc020043b"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "ğŸ¤– Agent:\n",
              "âœ… FACT: Tesla holds approximately 60-70% of the US electric vehicle (EV) market share as of early 2024."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Create a session instance\n",
        "# SQLite-based implementation of session storage.\n",
        "# This implementation stores conversation history in a SQLite database.\n",
        "\n",
        "session = SQLiteSession(\"conversation\")\n",
        "\n",
        "# First interaction\n",
        "user_input1 = \"What is the market share of Tesla in the US EV market?\"\n",
        "response1 = await Runner.run(\n",
        "    starting_agent = market_researcher_agent,\n",
        "    input = user_input1,\n",
        "    session = session,\n",
        ")\n",
        "\n",
        "print_markdown(f\"ğŸ¤– Agent:\\n{response1.final_output}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "awMAO5uml8G_",
        "outputId": "58a6f403-dfc4-412d-b329-892120106fb9"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "ğŸ¤– Agent:\n",
              "âœ… FACT: Tesla's US EV market share has slightly decreased from about 70-75% in 2023 to around 65-70% in early 2024 due to increasing competition."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Second interaction with history\n",
        "user_input2 = \"How does that compare to last year?\"\n",
        "\n",
        "response2 = await Runner.run(\n",
        "    starting_agent = market_researcher_agent,\n",
        "    input = user_input2,\n",
        "    session = session,\n",
        ")\n",
        "\n",
        "print_markdown(f\"ğŸ¤– Agent:\\n{response2.final_output}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ddBR29Xp5MrU"
      },
      "outputs": [],
      "source": [
        "import json  # Import the json module for handling JSON data\n",
        "from typing_extensions import TypedDict  # Import TypedDict for type hinting\n",
        "from agents import function_tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0k-vNqXGbTT_"
      },
      "outputs": [],
      "source": [
        "# Define a TypedDict for the expected parameters for the Tavily search function\n",
        "# A TypedDict is like a blueprint for a dictionary in Python\n",
        "# It tells Python exactly what keys the dictionary should have and what type of values go with each key.\n",
        "\n",
        "class TavilySearchParams(TypedDict):\n",
        "    query: str         # The search query string\n",
        "    max_results: int   # The maximum number of results to return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "FwW-udy-bXHh"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "@function_tool\n",
        "def tavily_search(params: TavilySearchParams) -> str:\n",
        "    \"\"\"\n",
        "    Calls the Tavily API and returns a string summary of top search results.\n",
        "\n",
        "    Args:\n",
        "        params (TavilySearchParams): Dictionary with 'query' (str) and 'max_results' (int).\n",
        "\n",
        "    Returns:\n",
        "        str: A formatted string summarizing the top search results, or an error message.\n",
        "    \"\"\"\n",
        "\n",
        "    # The web address (endpoint) for sending search requests to Tavily ( Tavily API endpoint)\n",
        "    url = \"https://api.tavily.com/search\"\n",
        "\n",
        "    # Tell the API that weâ€™re sending JSON data\n",
        "    headers = {\"Content-Type\": \"application/json\"}\n",
        "\n",
        "    # What weâ€™re sending to the API:\n",
        "    # Our secret API key (so Tavily knows it's us)\n",
        "    # The search text (query)\n",
        "    # How many results we want (defaults to 2 if not given)\n",
        "\n",
        "    payload = {\n",
        "        \"api_key\": tavily_api_key,  # Use the Tavily API key from environment\n",
        "        \"query\": params[\"query\"],   # The search query from params\n",
        "        \"max_results\": params.get(\"max_results\", 2),  # Use max_results from params, default to 2 if not provided\n",
        "    }\n",
        "\n",
        "    # Send the search request to Tavily (POST means weâ€™re sending data)\n",
        "    response = requests.post(url, json = payload, headers = headers)\n",
        "\n",
        "    # Check if the search worked (200 = OK)\n",
        "    if response.status_code == 200:  # If the request was successful\n",
        "        results = response.json().get(\"results\", [])  # Extract the 'results' list from the response JSON\n",
        "        print_markdown(f\"### ğŸ¤– Agentâ€™s Answer\\n{results}\")\n",
        "        # Build a summary string with each result's title and content, numbered\n",
        "        summary = \"\\n\".join([f\"{i+1}. {r['title']}: {r['content']}\" for i, r in enumerate(results)])\n",
        "        return summary if summary else \"No relevant results found.\"  # Return summary or fallback message\n",
        "    else:\n",
        "        return f\"Tavily API error: {response.status_code}\"  # Return error message with status code if request failed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "w8aHIn97dhZf"
      },
      "outputs": [],
      "source": [
        "from agents import SQLiteSession\n",
        "session = SQLiteSession(\"live_researcher_practice\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJPJXEsCdixF",
        "outputId": "d4e67715-d579-4766-9a31-fda90b469935"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Agent created with Tavily tool.\n"
          ]
        }
      ],
      "source": [
        "from agents import Agent\n",
        "\n",
        "live_researcher_agent = Agent(name = \"Live Market Researcher\",\n",
        "                             instructions = \"\"\"\n",
        "CONTEXT:\n",
        "You are a world-class market research assistant with access to real-time web search via the tavily_search tool.\n",
        "\n",
        "INSTRUCTION:\n",
        "- Analyze the user's question and determine if recent or real-time information is needed.\n",
        "- If the question involves recent events, news, or product info, always call tavily_search.\n",
        "- Summarize search results clearly and concisely, do not copy-paste.\n",
        "- Always start your answer with: \"ğŸ” According to a web search â€¦\"\n",
        "\n",
        "INPUT:\n",
        "You will receive a conversation history and the latest user question. Use the full context to inform your response.\n",
        "\n",
        "OUTPUT:\n",
        "Provide a clear, well-structured answer that references the search results when appropriate. If you use tavily_search, integrate the findings into your text summary only with 2-3 paragraphs.\n",
        "\"\"\",\n",
        "    model = \"gpt-4.1-mini\",\n",
        "    tools = [tavily_search])\n",
        "\n",
        "print(\"âœ… Agent created with Tavily tool.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "id": "I6MWpr-7dmST",
        "outputId": "9432dfd2-21fd-4762-ac69-64a62b9ef4e7"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**User:** What are people saying about the new GPT-5 Model?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "### ğŸ¤– Agentâ€™s Answer\n",
              "ğŸ” According to a web search, the new GPT-5 model is being received with a mix of excitement and thoughtful critique. People highlight its key innovation as a unified system that smartly routes requests to different sub-models based on the complexity and type of the task. This real-time router can adjust reasoning depth and effort, allowing GPT-5 to efficiently handle both straightforward and challenging problems without user intervention.\n",
              "\n",
              "The model is praised as OpenAI's strongest yet for coding tasks, notably for complex front-end code generation and debugging of large codebases. Users appreciate that GPT-5 \"just does stuff\" by deciding internally how much \"thinking\" to apply, which simplifies the user experience compared to earlier versions where users had to choose among several models manually.\n",
              "\n",
              "However, some more advanced users note that the model can be somewhat arbitrary in determining which problems require deeper reasoning, occasionally leading to longer processing times or inconsistent reasoning effort. Despite this, the consensus is that GPT-5 significantly reduces cognitive load for users, streamlines interaction with AI by hiding model selection complexities, and marks a major step forward in AI capability and usability."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from agents import Runner\n",
        "\n",
        "# First question\n",
        "q1 = \"What are people saying about the new GPT-5 Model?\"\n",
        "\n",
        "print_markdown(f\"**User:** {q1}\")\n",
        "\n",
        "run1 = await Runner.run(\n",
        "    starting_agent = live_researcher_agent,\n",
        "    input = q1,\n",
        "    session = session,\n",
        ")\n",
        "\n",
        "print_markdown(f\"### ğŸ¤– Agentâ€™s Answer\\n{run1.final_output}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 687
        },
        "id": "ZrCOzR4ozxZn",
        "outputId": "22e4b207-02a2-4f9b-b28c-2d4f6ee9b89a"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "### ğŸ¤– Agentâ€™s Answer\n",
              "[{'url': 'https://cleantechnica.com/2024/05/01/tesla-cybertruck-review-3000-miles-in-10-days/', 'title': 'Tesla Cybertruck Review â€” 3,000 Miles in 10 Days - CleanTechnica', 'content': '# Tesla Cybertruck Review â€” 3,000 Miles in 10 Days **Driving** â€” The Cybertruckâ€™s â€œscale weightâ€ is listed at 6,669 pounds on the vehicle registration. The Cybertruck drives better than my 2016 Model S â€” I was amazed by this fact! The Cybertruck accelerates like my Model S, even when passing trucks going up mountains on steep inclines. Perhaps it is the way we view the Cybertruck â€” as a utility vehicle that does â€œcity stuffâ€ but also goes off road, hauls dirty cargo (like sod and topsoil), and makes dump runs. **Range and Charging** â€” The Cybertruck when fully charged showed 318 miles of range.', 'score': 0.8452414, 'raw_content': None}, {'url': 'https://www.motortrend.com/news/2024-tesla-cybertruck-pros-cons-review', 'title': '2024 Tesla Cybertruck Pros and Cons Review: The No-Hype True ...', 'content': '# 2024 Tesla Cybertruck TOTY Review: The No-Hype, No-Spin True Truck Test *This review was conducted as part of our 2025 Truck of the Year (TOTY) testing, where each vehicle is evaluated on our six key criteria: efficiency, design, safety, engineering excellence, value, and performance of intended function. ### Tesla Cybertruck Beast is Both Quicker and Slower than Teslaâ€™s Claims in MotorTrend Testing ### 2024 Tesla Cybertruck Dual Motor Real-World Range Test: Beat by Ford and Rivian ### 2024 Tesla Cybertruck Dual Motor Foundation Series First Test: Youâ€™re All Wrong ### 2024 Tesla Cybertruck Dual Motor 0â€“60 MPH and Â¼-Mile Tested: Can It Cyberdust Other Electric Trucks?', 'score': 0.8036831, 'raw_content': None}, {'url': 'https://www.caranddriver.com/tesla/cybertruck', 'title': '2025 Tesla Cybertruck Review, Pricing, and Specs - Car and Driver', 'content': \"The Tesla Cybertruck is a giant pickup with silly-quick acceleration and a design that blows mindsâ€”and it's unexpectedly capable and nice to drive.\", 'score': 0.779339, 'raw_content': None}, {'url': 'https://www.reddit.com/r/teslamotors/comments/1hxo8fw/10000_mile_cybertruck_review/', 'title': '10,000 Mile Cybertruck Review : r/teslamotors - Reddit', 'content': '10,000 Mile Cybertruck Review : r/teslamotors Skip to main content10,000 Mile Cybertruck Review : r/teslamotors Image 1: r/teslamotors icon Go to teslamotors Image 3: r/teslamotors iconr/teslamotorsImage 4: bluecheckImage 5: bluecheck The original and largest Tesla community on Reddit! 10,000 Mile Cybertruck Review 10,000 mile Tesla Cybertruck review insights Best reviews for Tesla Cybertruck New to Reddit? Top Posts *   Reddit reReddit: Top posts of January 9, 2025 * * * *   Reddit reReddit: Top posts of January 2025 * * * *   Reddit reReddit: Top posts of 2025 * * * *   Reddit Meta *   Games  *   Gaming News & Discussion *   Other Games *   Action Movies & Series *   Animated Movies & Series *   About Reddit *   Best of Reddit', 'score': 0.7783298, 'raw_content': None}, {'url': 'https://electrek.co/2024/06/08/tesla-cybertruck-review-incredible-tech-packaged-weirdest-way/', 'title': 'Tesla Cybertruck review: incredible tech packaged in the weirdest way', 'content': 'It is a very novel vehicle right now, but Tesla is ramping up production and plans to make hundreds of thousands of Cybertrucks per year. I wouldnâ€™t be surprised if the vehicle program were born from a Tesla designer going up to CEO Elon Musk with the Cybertruckâ€™s design and Musk going: â€œI love it, letâ€™s figure out how to build this.â€ Then Teslaâ€™s engineers and designers had to make this thing a reality Moving to the interior design, thatâ€™s where I think that Tesla had to compromise a lot to make the exterior shape of the Cybertruck work. Instead, I now feel like the market is mostly existing Tesla owners who donâ€™t even really need a truck, but they are getting one because Tesla is making this.', 'score': 0.69845337, 'raw_content': None}]"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "### ğŸ¤– Agentâ€™s Answer\n",
              "ğŸ” According to a web search, reviews of the new Tesla Cybertruck highlight its impressive driving performance and distinctive design. Drivers have noted that the Cybertruck offers quick acceleration comparable to Tesla's Model S and performs well even on steep inclines. Some reviewers emphasize its utility, capable of city driving as well as off-road and hauling tasks, with a range of around 318 miles when fully charged.\n",
              "\n",
              "However, the unusual angular design and interior have been points of mixed opinions. While the exterior is eye-catching and futuristic, some feel that the interior shows compromises due to the unique shape. Reviewers also point out that the Cybertruck may attract mostly existing Tesla fans rather than traditional truck buyers. Overall, it is praised for its strong tech and performance, though some aspects like range and design generate debate compared to competing electric trucks."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from agents import Runner\n",
        "\n",
        "session = SQLiteSession(\"live_researcher_practice\")\n",
        "\n",
        "prod_memory = []\n",
        "\n",
        "q1 = \"What do reviwers say about the new Tesla CyberTruck\"\n",
        "\n",
        "run1 = await Runner.run(\n",
        "    starting_agent = live_researcher_agent,\n",
        "    input = q1,\n",
        "    session = session,\n",
        ")\n",
        "\n",
        "print_markdown(f\"### ğŸ¤– Agentâ€™s Answer\\n{run1.final_output}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "0KxK00-21I-o",
        "outputId": "7cdfa0b5-2200-4b72-a63a-9093f34f4162"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**User:** Summarize the main feature in one paragraph"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "### ğŸ¤– Agentâ€™s Answer\n",
              "The Tesla Cybertruck stands out with its striking, angular design and powerful performance, delivering quick acceleration comparable to the Model S and a versatile driving experience suited for both city and off-road use. It offers a range of around 318 miles on a full charge and strong towing and hauling capabilities, positioning it as a high-tech utility vehicle. While its futuristic exterior draws attention, the interior design reflects necessary compromises, appealing primarily to existing Tesla enthusiasts. Overall, it combines innovative technology, robust functionality, and a bold aesthetic that challenges traditional pickup truck norms."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "q2 = \"Summarize the main feature in one paragraph\"\n",
        "\n",
        "print_markdown(f\"**User:** {q2}\")\n",
        "\n",
        "run2 = await Runner.run(\n",
        "    starting_agent = live_researcher_agent,\n",
        "    input = q2,\n",
        "    session = session,\n",
        ")\n",
        "\n",
        "print_markdown(f\"### ğŸ¤– Agentâ€™s Answer\\n{run2.final_output}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
